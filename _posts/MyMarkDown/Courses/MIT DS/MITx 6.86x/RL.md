# RL

Instead of getting feedback for every action you are taking,

the agent will get the feedback at the very end.



### RL Terminology

still have possibility to go other directions (transition)



### Markov (MDP)

![image-20220805185022104](https://ik.imagekit.io/haochen/Typora/image-20220805185022104.png)

### Utility Function

![image-20220805190604484](https://ik.imagekit.io/haochen/Typora/image-20220805190604484.png)

### Bellman Equations

![image-20220806172906096](https://ik.imagekit.io/haochen/Typora/image-20220806172906096.png)

### Value Iteration

![image-20220806173745874](https://ik.imagekit.io/haochen/Typora/image-20220806173745874.png)

Converge to true value of the state

### Q-value Iteration

But we can also direct get **Q**

![image-20220806174922849](https://ik.imagekit.io/haochen/Typora/image-20220806174922849.png)

### Markovian Assumption

![image-20220806175517299](https://ik.imagekit.io/haochen/Typora/image-20220806175517299.png)

T is for transition function

![image-20220806180205224](https://ik.imagekit.io/haochen/Typora/image-20220806180205224.png)

### Estimating Inputs for RL algorithm

estimate of T and R because I tried stuff in the world.





