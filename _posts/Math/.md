# Method of Moments

![image-20220304232655126](https://chqwer2.github.io/img/Typora/image-20220304232655126.png)



Moment Generating Fun

![image-20220304233247913](https://chqwer2.github.io/img/Typora/image-20220304233247913.png)



![image-20220304235157190](https://chqwer2.github.io/img/Typora/image-20220304235157190.png)

Finally plug-in.

![image-20220305001319289](https://chqwer2.github.io/img/Typora/image-20220305001319289.png)

## Lecture 10 

Methods of Estimation: Method of Moments and M-Estimation

![image-20220306113508300](https://chqwer2.github.io/img/Typora/image-20220306113508300.png)

Simplify, Replacing expectations with averages.

we replace KL by other loss.

![image-20220306113711592](https://chqwer2.github.io/img/Typora/image-20220306113711592.png)

#### M-estimation

![image-20220306114159770](https://chqwer2.github.io/img/Typora/image-20220306114159770.png)

Now parameters not just param of distribution..

Now a function $\rho (\cdot)$,  and replace expectations with averages.

How do I find the $\rho$?





![image-20220306115746092](https://chqwer2.github.io/img/Typora/image-20220306115746092.png)

![image-20220306120335415](https://chqwer2.github.io/img/Typora/image-20220306120335415.png)

But absolute value is not differentiable?

How do we handle it. Why the $\rho$ is median?

it is a nice simmetric function, right?

**What if we tilt it a little bit?**

![image-20220306120905777](https://chqwer2.github.io/img/Typora/image-20220306120905777.png)



![image-20220306121208892](https://chqwer2.github.io/img/Typora/image-20220306121208892.png)

![image-20220306121318732](https://chqwer2.github.io/img/Typora/image-20220306121318732.png)





#  Preparations for the Asymptotic Normality of M-estimators





































### Recitation M-Estimation

M-estimation is the process of estimating

a parameter for a distribution using

some form of minimization.



So in this framework we want to minimize some loss function

to spit out an estimator of a parameter.



So with this sort of setup, what we want to do

is we want to look at M-estimators

for a couple of different distributions

and just sort of show how you can

get maybe desirable properties by choosing different loss

functions for rho.

avar for **asymtotic variance**

### sample median to the asymptotic variance

![image-20220306091826948](https://chqwer2.github.io/img/Typora/image-20220306091826948.png)



And we know sums of Bernoullis actually

do converge in distribution to a normal.

The only issue here is that the parameter in this Bernoulli

distribution will depend on the parameter n.



![image-20220306092556112](https://chqwer2.github.io/img/Typora/image-20220306092556112.png)



And you'll notice that this is actually just the definition

of the derivative of f, right?

![image-20220306093445838](https://chqwer2.github.io/img/Typora/image-20220306093445838.png)

In the next part, let's compare

that asymptotic variance that we just

found for the sample median to the asymptotic variance we would get for the sample mean.

### sample mean to the asymptotic variance

$m_n$ is the median

![image-20220306094734777](https://chqwer2.github.io/img/Typora/image-20220306094734777.png)

And this would give you some evidence

that using the sample median, at least for the Laplace

distribution, would give you better performance,

at least asymptotically, right?



We're going to do this again in the next one

and actually compare the median and mean as well as the Huber

estimator for the Cauchy distribution.

I do that by plugging in f of mu and showing that it equals 1/2.



So we note that f of mu actually does equal 1/2

because arc tangent of 0 is 0.

Right, so the true median of this distribution

is actually 1/2.



median = mean in this situation

![image-20220306095538235](https://chqwer2.github.io/img/Typora/image-20220306095538235.png)

whereas the asymptotic variance of the sample mean,

it just doesn't exist, right?

It doesn't even have a variance for one sample, or a mean.

Those integrals diverge.

So we've seen, at least, that the median's a better

estimator.

### Huber estimator 

![image-20220306095659568](https://chqwer2.github.io/img/Typora/image-20220306095659568.png)

differentiable



![image-20220306100353894](https://chqwer2.github.io/img/Typora/image-20220306100353894.png)

secant squared of x.



![image-20220306100847499](https://chqwer2.github.io/img/Typora/image-20220306100847499.png)

you think, OK, let's try to use a little b L'Hopital's rule, which you can actually do. 



So the asymptotic variance of the Huber M-estimator diverges.

And this corresponds to the case where

we're minimizing sums of squares, which is, again,

going back to the mean estimator, which

we showed earlier.



![image-20220306101616570](https://chqwer2.github.io/img/Typora/image-20220306101616570.png)

Sometimes the M-estimators have asymptotic variance

that doesn't exist, as is the case, when we look at the mean

under the kosher distribution.

But when they do exist, they give us

a good way of at least doing a simple calculation to see maybe

which estimate we should use in a given situation.